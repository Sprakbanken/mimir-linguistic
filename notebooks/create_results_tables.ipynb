{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create results tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "model_order = [\n",
    "    \"mimir-project/mimir-7b-books\",\n",
    "    \"mimir-project/mimir-7b-factual\",\n",
    "    \"mimir-project/mimir-7b-fiction\",\n",
    "    \"mimir-project/mimir-7b-newspapers\",\n",
    "    \"mimir-project/mimir-7b-nonfiction\",\n",
    "    \"mimir-project/mimir-7b-rightholders\",\n",
    "    \"mimir-project/mimir-7b-translated\",\n",
    "    \"mimir-project/mimir-7b-untranslated\",\n",
    "    \"mimir-project/mimir-7b-untranslated-withnewspapers\",\n",
    "    \"mimir-project/mimir-mistral-7b-base\",\n",
    "    \"mimir-project/mimir-mistral-7b-base-instruct\",\n",
    "    \"mimir-project/mimir-mistral-7b-base-scratch\",\n",
    "    \"mimir-project/mimir-mistral-7b-base-scratch-instruct\",\n",
    "    \"mimir-project/mimir-mistral-7b-extended\",\n",
    "    \"mimir-project/mimir-mistral-7b-extended-scratch\",\n",
    "    \"mimir-project/mimir-mistral-7b-extended-scratch-instruct\",\n",
    "    \"mimir-project/mimir-mistral-7b-extended-instruct\",\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mimir-project/mimir-mistral-1b-base-scratch\",\n",
    "    \"mimir-project/mimir-mistral-1b-extended-scratch\",\n",
    "    \"mimir-project/mimir-mistral-250m-base-scratch\",\n",
    "    \"mimir-project/mimir-mistral-250m-extended-scratch\",\n",
    "    \"mimir-project/mimir-mistral-7b-core-scratch\",\n",
    "    \"mimir-project/mimir-mistral-7b-core\",\n",
    "\n",
    "]\n",
    "\n",
    "scores_to_keep = [\n",
    "    \"compression_ratio_nob\",\n",
    "    \"compression_ratio_nno\",\n",
    "    \"lix_score_nob\",\n",
    "    \"lix_score_nno\",\n",
    "    \"self_bleu_nob\",\n",
    "    \"self_bleu_nno\",\n",
    "]\n",
    "\n",
    "\n",
    "def data_to_df(data: dict[str, str | float]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(data)\n",
    "    # only keep relevant columns\n",
    "    df = df[[\"model\"] + scores_to_keep]\n",
    "\n",
    "    # sort rows by model order\n",
    "    df[\"model\"] = pd.Categorical(df[\"model\"], categories=model_order, ordered=True)\n",
    "    df = df.sort_values(\"model\").reset_index(drop=True)\n",
    "\n",
    "    # rename model_column\n",
    "    df[\"model\"] = df.model.apply(\n",
    "        lambda x: x.split(\"/\")[1] if x.startswith(\"mimir-project\") else x\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def minmax_lix(lix: float) -> float:\n",
    "    return 100 * (60 - lix) / (60 - 20)\n",
    "\n",
    "\n",
    "def inverse_compression_ratio(comp_ratio: float) -> float:\n",
    "    return 100 * (1 / comp_ratio)\n",
    "\n",
    "\n",
    "def inverse_self_bleu(self_bleu: float) -> float:\n",
    "    return 100 * (1 - self_bleu)\n",
    "\n",
    "\n",
    "def normalize_and_rank(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Inverse compression ratio and self_bleu. Minmax lix. Add rank column based on inversed and minmax scores\"\"\"\n",
    "    df[\"inverse_compression_nob\"] = df.compression_ratio_nob.apply(\n",
    "        inverse_compression_ratio\n",
    "    )\n",
    "    df[\"inverse_compression_nno\"] = df.compression_ratio_nno.apply(\n",
    "        inverse_compression_ratio\n",
    "    )\n",
    "    df[\"min_max_lix_nob\"] = df.lix_score_nob.apply(minmax_lix)\n",
    "    df[\"min_max_lix_nno\"] = df.lix_score_nno.apply(minmax_lix)\n",
    "\n",
    "    df[\"inverse_sb_nob\"] = df.self_bleu_nob.apply(inverse_self_bleu)\n",
    "    df[\"inverse_sb_nno\"] = df.self_bleu_nno.apply(inverse_self_bleu)\n",
    "\n",
    "    df[\"average\"] = df[\n",
    "        [\n",
    "            \"inverse_compression_nob\",\n",
    "            \"inverse_compression_nno\",\n",
    "            \"min_max_lix_nob\",\n",
    "            \"min_max_lix_nno\",\n",
    "            \"inverse_sb_nob\",\n",
    "            \"inverse_sb_nno\",\n",
    "        ]\n",
    "    ].mean(axis=1)\n",
    "\n",
    "    df[\"rank\"] = df[\"average\"].rank(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create greedy table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_file_to_scores(results_file: Path) -> dict[str, float]:\n",
    "    df = pd.read_json(results_file, lines=True)\n",
    "    return {\n",
    "        k: v\n",
    "        for _, df_ in df.groupby(\"dataset\")\n",
    "        for k, v in df_.results.item()[0].items()\n",
    "        if k in scores_to_keep\n",
    "    }\n",
    "\n",
    "\n",
    "def get_data(data_path: Path) -> dict[str, list[str | float]]:\n",
    "    data = defaultdict(list)\n",
    "    for e in data_path.glob(\"*/*/\"):\n",
    "        if not e.is_dir:\n",
    "            continue\n",
    "        model_name = f\"{e.parent.name}/{e.name}\"\n",
    "        if model_name not in model_order:\n",
    "            continue\n",
    "\n",
    "        results_file = next(e.glob(\"evaluate_all/results.jsonl\"), None)\n",
    "        if results_file is None:\n",
    "            print(f\"No results file in {e}\")\n",
    "            continue\n",
    "        scores = results_file_to_scores(results_file)\n",
    "        data[\"model\"].append(model_name)\n",
    "        for score, value in scores.items():\n",
    "            data[score].append(value)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_10x_data(data_path: Path) -> dict[str, list[str | float]]:\n",
    "    \"\"\"Get average scores of 10 runs\"\"\"\n",
    "    data = defaultdict(list)\n",
    "    for e in data_path.glob(\"*/*/\"):\n",
    "        if not e.is_dir:\n",
    "            continue\n",
    "        model_name = f\"{e.parent.name}/{e.name}\"\n",
    "        if model_name not in model_order:\n",
    "            continue\n",
    "        results_files = list(e.glob(\"*/results.jsonl\"))\n",
    "        assert len(results_files) == 10\n",
    "        scores_sums = defaultdict(float)\n",
    "        for f in results_files:\n",
    "            scores = results_file_to_scores(f)\n",
    "            for k, v in scores.items():\n",
    "                scores_sums[k] += v\n",
    "        scores_avg = {k: v / 10 for k, v in scores_sums.items()}\n",
    "        data[\"model\"].append(model_name)\n",
    "        for score, value in scores_avg.items():\n",
    "            data[score].append(value)\n",
    "    return data\n",
    "\n",
    "\n",
    "greedy_p = Path(\"../results/greedy/\")\n",
    "data = get_data(greedy_p)\n",
    "df = data_to_df(data)\n",
    "df = normalize_and_rank(df)\n",
    "df.to_csv(greedy_p / \"results_table.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_10x_p = Path(\"../results/contrastive-10x/\")\n",
    "data = get_10x_data(contrastive_10x_p)\n",
    "df = data_to_df(data)\n",
    "df = normalize_and_rank(df)\n",
    "df.to_csv(contrastive_10x_p / \"results_table.csv\", index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mimir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
